{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from py.Training import training_and_testing, loss_function\n",
    "from py.vae import NeuralNet as nnet\n",
    "import py.config as CFG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms.functional as TF\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import random_split\n",
    "from torchinfo import summary\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "plt.rc('font', family='NanumGothic')\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rotate_images(image):\n",
    "    # 이미지를 0도, 90도, 180도, 270도로 회전시키는 함수\n",
    "    images = []\n",
    "    for angle in [0, 90, 180, 270]:\n",
    "        rotated_image = TF.rotate(image, angle)\n",
    "        images.append(rotated_image)\n",
    "    return images\n",
    "\n",
    "def gauss_noise(image_tensor, sigma=0.05):\n",
    "    # 이미지에 가우시안 노이즈를 추가하는 함수\n",
    "    noise = torch.randn(image_tensor.size()) * sigma\n",
    "    noisy_image = image_tensor + noise\n",
    "    noisy_image = torch.clamp(noisy_image, 0, 1)\n",
    "    return noisy_image\n",
    "\n",
    "\n",
    "class VAECustomDataset(Dataset):\n",
    "    def __init__(self, file_paths, transform=None, gauss_sigma=0.05):\n",
    "        # 데이터셋 초기화\n",
    "        self.file_paths = file_paths\n",
    "        self.transform = transform\n",
    "        self.gauss_sigma = gauss_sigma\n",
    "\n",
    "    def __len__(self):\n",
    "        # 데이터셋의 길이를 반환\n",
    "        return len(self.file_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # 주어진 인덱스에 해당하는 데이터를 반환\n",
    "        image = Image.open(self.file_paths[idx])\n",
    "        if self.transform:\n",
    "            # 변환이 주어진 경우\n",
    "            original_image = self.transform(image)  # 원본 이미지 변환\n",
    "            noisy_image = gauss_noise(original_image, self.gauss_sigma)  # 노이즈 추가 이미지 생성\n",
    "            images = rotate_images(image)  # 회전 이미지 생성\n",
    "            transformed_images = [self.transform(img) for img in images]  # 회전 이미지를 변환\n",
    "            noisy_images = [gauss_noise(img, self.gauss_sigma) for img in transformed_images]  # 회전된 이미지에 노이즈 추가\n",
    "            return original_image, noisy_image, transformed_images, noisy_images\n",
    "        else:\n",
    "            # 변환이 주어지지 않은 경우 원본 이미지 반환\n",
    "            return image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "trainset = torchvision.datasets.CIFAR10(root=CFG.pre_data_dir, train=True, download=True,\n",
    "                                        transform=CFG.transform_pre)\n",
    "train_loader = DataLoader(dataset=trainset,batch_size=CFG.batch_size, shuffle=True,\n",
    "                          num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 폴더 경로 설정\n",
    "folder = CFG.normal_root_dir\n",
    "file_path = glob.glob(os.path.join(folder, \"*.jpg\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(0.8 * len(file_path))\n",
    "test_size = len(file_path) - train_size\n",
    "\n",
    "train_paths, test_paths = random_split(file_path, [train_size, test_size])\n",
    "\n",
    "train_dataset = VAECustomDataset(file_paths=train_paths, transform=CFG.transform)\n",
    "test_dataset = VAECustomDataset(file_paths=test_paths, transform=CFG.transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of parameters: 968903145\n"
     ]
    }
   ],
   "source": [
    "neuralnet = nnet(height=CFG.height, width=CFG.width, channel=CFG.channel,\n",
    "                 device=CFG.device, ngpu=CFG.ngpu, ksize=CFG.ksize, z_dim=CFG.z_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "Encoder                                  [32, 20]                  --\n",
       "├─Sequential: 1-1                        [32, 64, 123, 123]        --\n",
       "│    └─Conv2d: 2-1                       [32, 16, 481, 481]        272\n",
       "│    └─ELU: 2-2                          [32, 16, 481, 481]        --\n",
       "│    └─Conv2d: 2-3                       [32, 16, 482, 482]        4,112\n",
       "│    └─ELU: 2-4                          [32, 16, 482, 482]        --\n",
       "│    └─MaxPool2d: 2-5                    [32, 16, 241, 241]        --\n",
       "│    └─Conv2d: 2-6                       [32, 32, 242, 242]        8,224\n",
       "│    └─ELU: 2-7                          [32, 32, 242, 242]        --\n",
       "│    └─Conv2d: 2-8                       [32, 32, 243, 243]        16,416\n",
       "│    └─ELU: 2-9                          [32, 32, 243, 243]        --\n",
       "│    └─MaxPool2d: 2-10                   [32, 32, 121, 121]        --\n",
       "│    └─Conv2d: 2-11                      [32, 64, 122, 122]        32,832\n",
       "│    └─ELU: 2-12                         [32, 64, 122, 122]        --\n",
       "│    └─Conv2d: 2-13                      [32, 64, 123, 123]        65,600\n",
       "│    └─ELU: 2-14                         [32, 64, 123, 123]        --\n",
       "├─Sequential: 1-2                        [32, 40]                  --\n",
       "│    └─Flatten: 2-15                     [32, 968256]              --\n",
       "│    └─Linear: 2-16                      [32, 512]                 495,747,584\n",
       "│    └─ELU: 2-17                         [32, 512]                 --\n",
       "│    └─Linear: 2-18                      [32, 40]                  20,520\n",
       "==========================================================================================\n",
       "Total params: 495,895,560\n",
       "Trainable params: 495,895,560\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.GIGABYTES): 142.28\n",
       "==========================================================================================\n",
       "Input size (MB): 29.49\n",
       "Forward/backward pass size (MB): 3354.61\n",
       "Params size (MB): 1983.58\n",
       "Estimated Total Size (MB): 5367.69\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(neuralnet.encoder, input_size=(32, 1, 480, 480), device='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "Decoder                                  [32, 1, 498, 498]         --\n",
       "├─Sequential: 1-1                        [32, 921600]              --\n",
       "│    └─Linear: 2-1                       [32, 512]                 10,752\n",
       "│    └─ELU: 2-2                          [32, 512]                 --\n",
       "│    └─Linear: 2-3                       [32, 921600]              472,780,800\n",
       "│    └─ELU: 2-4                          [32, 921600]              --\n",
       "├─Sequential: 1-2                        [32, 1, 498, 498]         --\n",
       "│    └─Conv2d: 2-5                       [32, 64, 121, 121]        65,600\n",
       "│    └─ELU: 2-6                          [32, 64, 121, 121]        --\n",
       "│    └─Conv2d: 2-7                       [32, 64, 122, 122]        65,600\n",
       "│    └─ELU: 2-8                          [32, 64, 122, 122]        --\n",
       "│    └─ConvTranspose2d: 2-9              [32, 32, 246, 246]        51,232\n",
       "│    └─ELU: 2-10                         [32, 32, 246, 246]        --\n",
       "│    └─Conv2d: 2-11                      [32, 32, 247, 247]        16,416\n",
       "│    └─ELU: 2-12                         [32, 32, 247, 247]        --\n",
       "│    └─ConvTranspose2d: 2-13             [32, 16, 496, 496]        12,816\n",
       "│    └─ELU: 2-14                         [32, 16, 496, 496]        --\n",
       "│    └─Conv2d: 2-15                      [32, 16, 497, 497]        4,112\n",
       "│    └─ELU: 2-16                         [32, 16, 497, 497]        --\n",
       "│    └─Conv2d: 2-17                      [32, 1, 498, 498]         257\n",
       "│    └─Sigmoid: 2-18                     [32, 1, 498, 498]         --\n",
       "==========================================================================================\n",
       "Total params: 473,007,585\n",
       "Trainable params: 473,007,585\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.GIGABYTES): 343.80\n",
       "==========================================================================================\n",
       "Input size (MB): 0.00\n",
       "Forward/backward pass size (MB): 3798.25\n",
       "Params size (MB): 1892.03\n",
       "Estimated Total Size (MB): 5690.28\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(neuralnet.decoder, input_size=(32, 20), device=CFG.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretrain(neuralnet, train_loader, epochs):\n",
    "    neuralnet.train(mode=True)\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        for i, (inputs, _) in enumerate(train_loader):\n",
    "            input = inputs.to(device=CFG.device)\n",
    "            enc, mu, sigma = neuralnet.encoder(input)\n",
    "            x_hat = neuralnet.decoder(enc)\n",
    "            loss = loss_function(\n",
    "                x=input, x_hat=x_hat, mu=mu, sigma=sigma\n",
    "            )\n",
    "            neuralnet.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            neuralnet.optimizer.step()\n",
    "            \n",
    "            if i % 10 == 0:\n",
    "                print(f'Epoch : [{epoch+1}/{epochs}], Loss: {loss.item():.4f}')\n",
    "    torch.save(neuralnet.state_dict(), 'vae_cifar10_pretrained.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "pretrain(neuralnet=neuralnet, train_loader=train_loader, epochs=CFG.epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_and_testing(neuralnet=neuralnet ,train_dataset=train_dataset, test_dataset= test_dataset,\n",
    "                      epochs=CFG.epochs, batch_size=CFG.batch_size)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
