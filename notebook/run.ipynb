{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from py.Training import training_and_testing, loss_function\n",
    "from py.vae import NeuralNet as nnet\n",
    "import py.config as CFG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms.functional as TF\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import random_split\n",
    "from torchinfo import summary\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "plt.rc('font', family='NanumGothic')\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rotate_images(image):\n",
    "    # 이미지를 0도, 90도, 180도, 270도로 회전시키는 함수\n",
    "    images = []\n",
    "    for angle in [0, 90, 180, 270]:\n",
    "        rotated_image = TF.rotate(image, angle)\n",
    "        images.append(rotated_image)\n",
    "    return images\n",
    "\n",
    "def gauss_noise(image_tensor, sigma=0.05):\n",
    "    # 이미지에 가우시안 노이즈를 추가하는 함수\n",
    "    noise = torch.randn(image_tensor.size()) * sigma\n",
    "    noisy_image = image_tensor + noise\n",
    "    noisy_image = torch.clamp(noisy_image, 0, 1)\n",
    "    return noisy_image\n",
    "\n",
    "\n",
    "class VAECustomDataset(Dataset):\n",
    "    def __init__(self, file_paths, transform=None, gauss_sigma=0.05):\n",
    "        # 데이터셋 초기화\n",
    "        self.file_paths = file_paths\n",
    "        self.transform = transform\n",
    "        self.gauss_sigma = gauss_sigma\n",
    "\n",
    "    def __len__(self):\n",
    "        # 데이터셋의 길이를 반환\n",
    "        return len(self.file_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # 주어진 인덱스에 해당하는 데이터를 반환\n",
    "        image = Image.open(self.file_paths[idx])\n",
    "        if self.transform:\n",
    "            # 변환이 주어진 경우\n",
    "            original_image = self.transform(image)  # 원본 이미지 변환\n",
    "            noisy_image = gauss_noise(original_image, self.gauss_sigma)  # 노이즈 추가 이미지 생성\n",
    "            images = rotate_images(image)  # 회전 이미지 생성\n",
    "            transformed_images = [self.transform(img) for img in images]  # 회전 이미지를 변환\n",
    "            noisy_images = [gauss_noise(img, self.gauss_sigma) for img in transformed_images]  # 회전된 이미지에 노이즈 추가\n",
    "            return original_image, noisy_image, transformed_images, noisy_images\n",
    "        else:\n",
    "            # 변환이 주어지지 않은 경우 원본 이미지 반환\n",
    "            return image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "trainset = torchvision.datasets.CIFAR10(root='D:\\PPJ\\Model\\pre_data', train=True, download=True,\n",
    "                                        transform=CFG.transform_pre)\n",
    "train_loader = DataLoader(dataset=trainset,batch_size=CFG.batch_size, shuffle=True,\n",
    "                          num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 폴더 경로 설정\n",
    "folder = r\"D:\\PPJ\\Model\\data\\Normal\"\n",
    "file_path = glob.glob(os.path.join(folder, \"*.jpg\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(0.8 * len(file_path))\n",
    "test_size = len(file_path) - train_size\n",
    "\n",
    "train_paths, test_paths = random_split(file_path, [train_size, test_size])\n",
    "\n",
    "train_dataset = VAECustomDataset(file_paths=train_paths, transform=CFG.transform)\n",
    "test_dataset = VAECustomDataset(file_paths=test_paths, transform=CFG.transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of parameters: 968903145\n"
     ]
    }
   ],
   "source": [
    "neuralnet = nnet(height=CFG.height, width=CFG.width, channel=CFG.channel,\n",
    "                 device=CFG.device, ngpu=CFG.ngpu, ksize=CFG.ksize, z_dim=CFG.z_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 20])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "Encoder                                  [32, 20]                  --\n",
       "├─Sequential: 1-1                        [32, 64, 123, 123]        --\n",
       "│    └─Conv2d: 2-1                       [32, 16, 481, 481]        272\n",
       "│    └─ELU: 2-2                          [32, 16, 481, 481]        --\n",
       "│    └─Conv2d: 2-3                       [32, 16, 482, 482]        4,112\n",
       "│    └─ELU: 2-4                          [32, 16, 482, 482]        --\n",
       "│    └─MaxPool2d: 2-5                    [32, 16, 241, 241]        --\n",
       "│    └─Conv2d: 2-6                       [32, 32, 242, 242]        8,224\n",
       "│    └─ELU: 2-7                          [32, 32, 242, 242]        --\n",
       "│    └─Conv2d: 2-8                       [32, 32, 243, 243]        16,416\n",
       "│    └─ELU: 2-9                          [32, 32, 243, 243]        --\n",
       "│    └─MaxPool2d: 2-10                   [32, 32, 121, 121]        --\n",
       "│    └─Conv2d: 2-11                      [32, 64, 122, 122]        32,832\n",
       "│    └─ELU: 2-12                         [32, 64, 122, 122]        --\n",
       "│    └─Conv2d: 2-13                      [32, 64, 123, 123]        65,600\n",
       "│    └─ELU: 2-14                         [32, 64, 123, 123]        --\n",
       "├─Sequential: 1-2                        [32, 40]                  --\n",
       "│    └─Flatten: 2-15                     [32, 968256]              --\n",
       "│    └─Linear: 2-16                      [32, 512]                 495,747,584\n",
       "│    └─ELU: 2-17                         [32, 512]                 --\n",
       "│    └─Linear: 2-18                      [32, 40]                  20,520\n",
       "==========================================================================================\n",
       "Total params: 495,895,560\n",
       "Trainable params: 495,895,560\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.GIGABYTES): 142.28\n",
       "==========================================================================================\n",
       "Input size (MB): 29.49\n",
       "Forward/backward pass size (MB): 3354.61\n",
       "Params size (MB): 1983.58\n",
       "Estimated Total Size (MB): 5367.69\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(neuralnet.encoder, input_size=(32, 1, 480, 480), device='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "Decoder                                  [32, 1, 495, 495]         --\n",
       "├─Sequential: 1-1                        [32, 921600]              --\n",
       "│    └─Linear: 2-1                       [32, 512]                 10,752\n",
       "│    └─ELU: 2-2                          [32, 512]                 --\n",
       "│    └─Linear: 2-3                       [32, 921600]              472,780,800\n",
       "│    └─ELU: 2-4                          [32, 921600]              --\n",
       "├─Sequential: 1-2                        [32, 1, 495, 495]         --\n",
       "│    └─Conv2d: 2-5                       [32, 64, 121, 121]        65,600\n",
       "│    └─ELU: 2-6                          [32, 64, 121, 121]        --\n",
       "│    └─Conv2d: 2-7                       [32, 64, 122, 122]        65,600\n",
       "│    └─ELU: 2-8                          [32, 64, 122, 122]        --\n",
       "│    └─ConvTranspose2d: 2-9              [32, 32, 245, 245]        51,232\n",
       "│    └─ELU: 2-10                         [32, 32, 245, 245]        --\n",
       "│    └─Conv2d: 2-11                      [32, 32, 246, 246]        16,416\n",
       "│    └─ELU: 2-12                         [32, 32, 246, 246]        --\n",
       "│    └─ConvTranspose2d: 2-13             [32, 16, 493, 493]        12,816\n",
       "│    └─ELU: 2-14                         [32, 16, 493, 493]        --\n",
       "│    └─Conv2d: 2-15                      [32, 16, 494, 494]        4,112\n",
       "│    └─ELU: 2-16                         [32, 16, 494, 494]        --\n",
       "│    └─Conv2d: 2-17                      [32, 1, 495, 495]         257\n",
       "│    └─Sigmoid: 2-18                     [32, 1, 495, 495]         --\n",
       "==========================================================================================\n",
       "Total params: 473,007,585\n",
       "Trainable params: 473,007,585\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.GIGABYTES): 341.11\n",
       "==========================================================================================\n",
       "Input size (MB): 0.00\n",
       "Forward/backward pass size (MB): 3765.10\n",
       "Params size (MB): 1892.03\n",
       "Estimated Total Size (MB): 5657.13\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(neuralnet.decoder, input_size=(32, 20), device=CFG.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretrain(neuralnet, train_loader, epochs):\n",
    "    neuralnet.train(mode=True)\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        for i, (inputs, _) in enumerate(train_loader):\n",
    "            input = inputs.to(device=CFG.device)\n",
    "            enc = mu, sigma = neuralnet.encoder(input)\n",
    "            x_hat = neuralnet.decoder(enc)\n",
    "            loss = loss_function(\n",
    "                x=input, x_hat=x_hat, mu=mu, sigma=sigma\n",
    "            )\n",
    "            neuralnet.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            neuralnet.optimizer.step()\n",
    "            \n",
    "            if i % 10 == 0:\n",
    "                print(f'Epoch : [{epoch+1}/{epochs}], Loss: {loss.item():.4f}')\n",
    "    torch.save(neuralnet.state_dict(), 'vae_cifar10_pretrained.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100 [00:14<?, ?it/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mpretrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mneuralnet\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mneuralnet\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mCFG\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[9], line 6\u001b[0m, in \u001b[0;36mpretrain\u001b[1;34m(neuralnet, train_loader, epochs)\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, (inputs, _) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_loader):\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m inputs\u001b[38;5;241m.\u001b[39mto(device\u001b[38;5;241m=\u001b[39mCFG\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m----> 6\u001b[0m     enc \u001b[38;5;241m=\u001b[39m mu, sigma \u001b[38;5;241m=\u001b[39m neuralnet\u001b[38;5;241m.\u001b[39mencoder(\u001b[38;5;28minput\u001b[39m)\n\u001b[0;32m      7\u001b[0m     x_hat \u001b[38;5;241m=\u001b[39m neuralnet\u001b[38;5;241m.\u001b[39mdecoder(enc)\n\u001b[0;32m      8\u001b[0m     loss \u001b[38;5;241m=\u001b[39m loss_function(\n\u001b[0;32m      9\u001b[0m         x\u001b[38;5;241m=\u001b[39m\u001b[38;5;28minput\u001b[39m, x_hat\u001b[38;5;241m=\u001b[39mx_hat, mu\u001b[38;5;241m=\u001b[39mmu, sigma\u001b[38;5;241m=\u001b[39msigma\n\u001b[0;32m     10\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "pretrain(neuralnet=neuralnet, train_loader=train_loader, epochs=CFG.epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NeuralNet' object has no attribute 'train'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[32], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtraining_and_testing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mneuralnet\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mneuralnet\u001b[49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtest_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[43m                      \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mCFG\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mCFG\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\PPJ\\Model\\notebook\\py\\Training.py:153\u001b[0m, in \u001b[0;36mtraining_and_testing\u001b[1;34m(neuralnet, train_dataset, test_dataset, epochs, batch_size)\u001b[0m\n\u001b[0;32m    150\u001b[0m test_loader \u001b[38;5;241m=\u001b[39m prepare_test_loader(test_dataset, batch_size)\n\u001b[0;32m    152\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[1;32m--> 153\u001b[0m     list_recon, list_kld, list_total, iteration \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    154\u001b[0m \u001b[43m        \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mneuralnet\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwriter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miteration\u001b[49m\n\u001b[0;32m    155\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    157\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m], Total Iteration: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00miteration\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    158\u001b[0m           \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRestore Error: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28msum\u001b[39m(list_recon)\u001b[38;5;241m/\u001b[39m\u001b[38;5;28mlen\u001b[39m(list_recon)\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    159\u001b[0m           \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mKLD: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28msum\u001b[39m(list_kld)\u001b[38;5;241m/\u001b[39m\u001b[38;5;28mlen\u001b[39m(list_kld)\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    160\u001b[0m           \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTotal Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28msum\u001b[39m(list_total)\u001b[38;5;241m/\u001b[39m\u001b[38;5;28mlen\u001b[39m(list_total)\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    162\u001b[0m     \u001b[38;5;66;03m#model save\u001b[39;00m\n",
      "File \u001b[1;32md:\\PPJ\\Model\\notebook\\py\\Training.py:64\u001b[0m, in \u001b[0;36mtrain_epoch\u001b[1;34m(epoch, neuralnet, train_loader, writer, iteration)\u001b[0m\n\u001b[0;32m     63\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain_epoch\u001b[39m(epoch, neuralnet, train_loader, writer, iteration):\n\u001b[1;32m---> 64\u001b[0m     \u001b[43mneuralnet\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m()\n\u001b[0;32m     65\u001b[0m     list_recon, list_kld, list_total \u001b[38;5;241m=\u001b[39m [], [], []\n\u001b[0;32m     67\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m batch_idx, (original_image, noisy_image, transformed_images, noisy_images) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_loader):\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NeuralNet' object has no attribute 'train'"
     ]
    }
   ],
   "source": [
    "training_and_testing(neuralnet=neuralnet ,train_dataset=train_dataset, test_dataset= test_dataset,\n",
    "                      epochs=CFG.epochs, batch_size=CFG.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
